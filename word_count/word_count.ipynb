{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23b9a7c",
   "metadata": {},
   "source": [
    "## Define mapper map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bf45a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper.py\n",
    "import io\n",
    "import sys\n",
    "\n",
    "input_stream = io.TextIOWrapper(sys.stdin.buffer, encoding='latin1')\n",
    "\n",
    "# Your code setting up data structures here if necessary (equivalent to setup() function)\n",
    "\n",
    "for line in input_stream:\n",
    "    # Your code operating on each line here (equivalent to map() function)\n",
    "    for word in line.strip().split():\n",
    "        # (word, 1)\n",
    "        print(f\"{word}\\t1\")\n",
    "\n",
    "# Your code for post-processing here if necessary (equivalent to cleanup() function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5ebd4",
   "metadata": {},
   "source": [
    "## Define reducer reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22e9827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer.py\n",
    "import sys\n",
    "\n",
    "# Your code setting up data structures here if necessary (equivalent to setup() function)\n",
    "table = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Your code operating on each line here (equivalent to map() function)\n",
    "    print(f'{line}', file=sys.stderr)\n",
    "    word, count = line.strip().split('\\t')\n",
    "    count = int(count)\n",
    "    if word not in table:\n",
    "        table[word] = count\n",
    "    else:\n",
    "        table[word] += count\n",
    "\n",
    "# Your code for post-processing here if necessary (equivalent to cleanup() function)\n",
    "for word, count in table.items():\n",
    "    print(f\"{word}\\t{count}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa61579",
   "metadata": {},
   "source": [
    "## Define combiner (part of mapper) combine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c2267f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combiner is the same as reducer, becuase it is an associative operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "266041a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+rwx mapper.py\n",
    "!chmod u+rwx reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe836425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting doc.txt\n"
     ]
    }
   ],
   "source": [
    "%%file doc.txt\n",
    "hello hello\n",
    "hadoop so good\n",
    "hello world\n",
    "hadoop good\n",
    "hell world\n",
    "hadoop spark\n",
    "hello spark spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99446120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/neteasegames/Desktop/hadoop_mapreduce/word_count\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_directory = os.path.abspath(os.getcwd())\n",
    "print(\"Notebook directory:\", notebook_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aacea5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.join(notebook_directory, 'output')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f77af596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-01 11:35:44,140 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "2024-05-01 11:35:44,241 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/neteasegames/Desktop/hadoop_mapreduce/word_count/mapper.py, /Users/neteasegames/Desktop/hadoop_mapreduce/word_count/reducer.py, /var/folders/j4/j3x8v2856p95gl278v7rl4j80000gn/T/hadoop-unjar10345594873227815573/] [] /var/folders/j4/j3x8v2856p95gl278v7rl4j80000gn/T/streamjob839306021278679894.jar tmpDir=null\n",
      "2024-05-01 11:35:44,736 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-01 11:35:44,860 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-01 11:35:45,065 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/neteasegames/.staging/job_1714527448434_0016\n",
      "2024-05-01 11:35:45,765 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-05-01 11:35:45,815 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-05-01 11:35:45,945 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1714527448434_0016\n",
      "2024-05-01 11:35:45,945 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-05-01 11:35:46,060 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-05-01 11:35:46,060 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-05-01 11:35:46,113 INFO impl.YarnClientImpl: Submitted application application_1714527448434_0016\n",
      "2024-05-01 11:35:46,131 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1714527448434_0016/\n",
      "2024-05-01 11:35:46,132 INFO mapreduce.Job: Running job: job_1714527448434_0016\n",
      "2024-05-01 11:35:51,228 INFO mapreduce.Job: Job job_1714527448434_0016 running in uber mode : false\n",
      "2024-05-01 11:35:51,230 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-05-01 11:35:56,348 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-05-01 11:36:00,390 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-05-01 11:36:01,419 INFO mapreduce.Job: Job job_1714527448434_0016 completed successfully\n",
      "2024-05-01 11:36:01,531 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=243\n",
      "\t\tFILE: Number of bytes written=940583\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=240\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=2\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4222\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1711\n",
      "\t\tTotal time spent by all map tasks (ms)=4222\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1711\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4222\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1711\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4323328\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1752064\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=125\n",
      "\t\tMap output materialized bytes=109\n",
      "\t\tInput split bytes=240\n",
      "\t\tCombine input records=16\n",
      "\t\tCombine output records=10\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=109\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=7\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=805306368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=140\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=64\n",
      "2024-05-01 11:36:01,531 INFO streaming.StreamJob: Output directory: file:///Users/neteasegames/Desktop/hadoop_mapreduce/word_count/output\n"
     ]
    }
   ],
   "source": [
    "# brew install hadoop\n",
    "!hadoop jar /opt/homebrew/Cellar/hadoop/3.4.0/libexec/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-input file://{os.path.join(notebook_directory, 'doc.txt')} \\\n",
    "-output file://{os.path.join(notebook_directory, 'output')} \\\n",
    "-file {os.path.join(notebook_directory, 'mapper.py')} \\\n",
    "-file {os.path.join(notebook_directory, 'reducer.py')} \\\n",
    "-mapper '/Users/neteasegames/anaconda3/bin/python mapper.py' \\\n",
    "-reducer '/Users/neteasegames/anaconda3/bin/python reducer.py' \\\n",
    "-combiner '/Users/neteasegames/anaconda3/bin/python reducer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "087f59bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\t2\r\n",
      "hadoop\t3\r\n",
      "hell\t1\r\n",
      "hello\t4\r\n",
      "so\t1\r\n",
      "spark\t3\r\n",
      "world\t2\r\n"
     ]
    }
   ],
   "source": [
    "# 00000 is the reducer\n",
    "!cat output/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954fe773",
   "metadata": {},
   "source": [
    "## Mapper 1 Combiner Input\n",
    "hadoop\t1\n",
    "\n",
    "hell\t1\n",
    "\n",
    "hello\t1\n",
    "\n",
    "spark\t1\n",
    "\n",
    "spark\t1\n",
    "\n",
    "spark\t1\n",
    "\n",
    "world\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed294b53",
   "metadata": {},
   "source": [
    "## Mapper 2 Combiner Input\n",
    "good\t1\n",
    "\n",
    "good\t1\n",
    "\n",
    "hadoop\t1\n",
    "\n",
    "hadoop\t1\n",
    "\n",
    "hello\t1\n",
    "\n",
    "hello\t1\n",
    "\n",
    "hello\t1\n",
    "\n",
    "so\t1\n",
    "\n",
    "world\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4093a283",
   "metadata": {},
   "source": [
    "## Reducer input\n",
    "good\t2\n",
    "\n",
    "hadoop\t2\n",
    "\n",
    "hadoop\t1\n",
    "\n",
    "hell\t1\n",
    "\n",
    "hello\t1\n",
    "\n",
    "hello\t3\n",
    "\n",
    "so\t1\n",
    "\n",
    "spark\t3\n",
    "\n",
    "world\t1\n",
    "\n",
    "world\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362d8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
