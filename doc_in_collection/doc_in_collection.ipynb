{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3dd2ef1",
   "metadata": {},
   "source": [
    "## Define mapper map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9bf45a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper.py\n",
    "import io\n",
    "import sys\n",
    "\n",
    "input_stream = io.TextIOWrapper(sys.stdin.buffer, encoding='latin1')\n",
    "\n",
    "# Your code setting up data structures here if necessary (equivalent to setup() function)\n",
    "\n",
    "for line in input_stream:\n",
    "    # Your code operating on each line here (equivalent to map() function)\n",
    "    collectionID, doc = line.strip().split()\n",
    "    print(f\"{doc}\\t{collectionID}\")\n",
    "\n",
    "# Your code for post-processing here if necessary (equivalent to cleanup() function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59842e0",
   "metadata": {},
   "source": [
    "## Define reducer reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "22e9827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer.py\n",
    "import sys\n",
    "\n",
    "# Your code setting up data structures here if necessary (equivalent to setup() function)\n",
    "table = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Your code operating on each line here (more or less equivalent to reduce() function,\n",
    "    # but only operating on a single tuple at a time), pairs are sorted by key\n",
    "    print(f'{line}', file=sys.stderr)\n",
    "    doc, collectionID = line.strip().split('\\t')\n",
    "    collectionID = int(collectionID)\n",
    "    if doc not in table:\n",
    "        table[doc] = [collectionID]\n",
    "    else:\n",
    "        table[doc] += [collectionID]\n",
    "\n",
    "for doc, collectionIDs in table.items():\n",
    "    if 0 in collectionIDs and 1 in collectionIDs:\n",
    "        print(f\"{doc}\\t1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa8c49",
   "metadata": {},
   "source": [
    "## Define combiner (part of mapper) combine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c2267f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%file combiner.py\n",
    "import sys\n",
    "\n",
    "# Your code setting up data structures here if necessary (equivalent to setup() function)\n",
    "table = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Your code operating on each line here (more or less equivalent to reduce() function,\n",
    "    # but only operating on a single tuple at a time), pairs are sorted by key\n",
    "    print(f'{line}', file=sys.stderr)\n",
    "    doc, collectionID = line.strip().split('\\t')\n",
    "    collectionID = int(collectionID)\n",
    "    if doc not in table:\n",
    "        table[doc] = [collectionID]\n",
    "    else:\n",
    "        table[doc] += [collectionID]\n",
    "\n",
    "for doc, collectionIDs in table.items():\n",
    "    if 0 in collectionIDs:\n",
    "        print(f\"{doc}\\t0\")\n",
    "    if 1 in collectionIDs:\n",
    "        print(f\"{doc}\\t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "266041a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+rwx mapper.py\n",
    "!chmod u+rwx reducer.py\n",
    "!chmod u+rwx combiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fe836425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting doc.txt\n"
     ]
    }
   ],
   "source": [
    "%%file doc.txt\n",
    "0 hellohello\n",
    "0 hadoop\n",
    "0 hadoop\n",
    "1 hello\n",
    "1 hadoop\n",
    "0 hell\n",
    "0 hadoop\n",
    "0 hello\n",
    "1 hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "99446120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/neteasegames/Desktop/hadoop_mapreduce/doc_in_collection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_directory = os.path.abspath(os.getcwd())\n",
    "print(\"Notebook directory:\", notebook_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aacea5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.join(notebook_directory, 'output')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f77af596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-01 11:27:01,456 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "2024-05-01 11:27:01,566 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/neteasegames/Desktop/hadoop_mapreduce/doc_in_collection/mapper.py, /Users/neteasegames/Desktop/hadoop_mapreduce/doc_in_collection/reducer.py, /Users/neteasegames/Desktop/hadoop_mapreduce/doc_in_collection/combiner.py, /var/folders/j4/j3x8v2856p95gl278v7rl4j80000gn/T/hadoop-unjar7771402297968905089/] [] /var/folders/j4/j3x8v2856p95gl278v7rl4j80000gn/T/streamjob12112375089825970283.jar tmpDir=null\n",
      "2024-05-01 11:27:02,069 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-01 11:27:02,187 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-01 11:27:02,350 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/neteasegames/.staging/job_1714527448434_0015\n",
      "2024-05-01 11:27:03,083 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-05-01 11:27:03,567 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-05-01 11:27:03,721 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1714527448434_0015\n",
      "2024-05-01 11:27:03,721 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-05-01 11:27:03,839 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-05-01 11:27:03,839 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-05-01 11:27:03,894 INFO impl.YarnClientImpl: Submitted application application_1714527448434_0015\n",
      "2024-05-01 11:27:03,912 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1714527448434_0015/\n",
      "2024-05-01 11:27:03,913 INFO mapreduce.Job: Running job: job_1714527448434_0015\n",
      "2024-05-01 11:27:09,041 INFO mapreduce.Job: Job job_1714527448434_0015 running in uber mode : false\n",
      "2024-05-01 11:27:09,043 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-05-01 11:27:13,139 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-05-01 11:27:18,186 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-05-01 11:27:19,213 INFO mapreduce.Job: Job job_1714527448434_0015 completed successfully\n",
      "2024-05-01 11:27:19,326 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=216\n",
      "\t\tFILE: Number of bytes written=941658\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=254\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=2\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4060\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1722\n",
      "\t\tTotal time spent by all map tasks (ms)=4060\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1722\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4060\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1722\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4157440\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1763328\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=9\n",
      "\t\tMap output records=9\n",
      "\t\tMap output bytes=81\n",
      "\t\tMap output materialized bytes=100\n",
      "\t\tInput split bytes=254\n",
      "\t\tCombine input records=9\n",
      "\t\tCombine output records=8\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=100\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=805306368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=122\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=29\n",
      "2024-05-01 11:27:19,326 INFO streaming.StreamJob: Output directory: file:///Users/neteasegames/Desktop/hadoop_mapreduce/doc_in_collection/output\n"
     ]
    }
   ],
   "source": [
    "# brew install hadoop\n",
    "!hadoop jar /opt/homebrew/Cellar/hadoop/3.4.0/libexec/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-input file://{os.path.join(notebook_directory, 'doc.txt')} \\\n",
    "-output file://{os.path.join(notebook_directory, 'output')} \\\n",
    "-file {os.path.join(notebook_directory, 'mapper.py')} \\\n",
    "-file {os.path.join(notebook_directory, 'reducer.py')} \\\n",
    "-file {os.path.join(notebook_directory, 'combiner.py')} \\\n",
    "-mapper '/Users/neteasegames/anaconda3/bin/python mapper.py' \\\n",
    "-reducer '/Users/neteasegames/anaconda3/bin/python reducer.py' \\\n",
    "-combiner '/Users/neteasegames/anaconda3/bin/python combiner.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "087f59bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\t1\r\n",
      "hello\t1\r\n"
     ]
    }
   ],
   "source": [
    "# 00000 is the reducer\n",
    "!cat output/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f668ea",
   "metadata": {},
   "source": [
    "## Mapper 1 Combiner Input\n",
    "hadoop\t1\n",
    "\n",
    "hadoop\t0\n",
    "\n",
    "hell\t0\n",
    "\n",
    "hello\t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d2c0e",
   "metadata": {},
   "source": [
    "## Mapper 2 Combiner Input\n",
    "hadoop\t1\n",
    "\n",
    "hadoop\t0\n",
    "\n",
    "hadoop\t0\n",
    "\n",
    "hello\t1\n",
    "\n",
    "hellohello\t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc25299",
   "metadata": {},
   "source": [
    "# Reduce Input\n",
    "hadoop\t0\n",
    "\n",
    "hadoop\t1\n",
    "\n",
    "hadoop\t0\n",
    "\n",
    "hadoop\t1\n",
    "\n",
    "hell\t0\n",
    "\n",
    "hello\t0\n",
    "\n",
    "hello\t1\n",
    "\n",
    "hellohello\t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae2f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
