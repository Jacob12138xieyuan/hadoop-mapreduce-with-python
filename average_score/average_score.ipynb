{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23b9a7c",
   "metadata": {},
   "source": [
    "## Define mapper map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf45a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper.py\n",
    "import io\n",
    "import sys\n",
    "\n",
    "input_stream = io.TextIOWrapper(sys.stdin.buffer, encoding='latin1')\n",
    "\n",
    "# Your code setting up data structures here if necessary (equivalent to setup() function)\n",
    "\n",
    "for line in input_stream:\n",
    "    # Your code operating on each line here (equivalent to map() function)\n",
    "    name, score = line.strip().split()\n",
    "    # (name, score_sum, count)\n",
    "    print(f\"{name}\\t{score}\\t1\")\n",
    "\n",
    "# Your code for post-processing here if necessary (equivalent to cleanup() function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5ebd4",
   "metadata": {},
   "source": [
    "## Define reducer reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e9827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer.py\n",
    "import sys\n",
    "\n",
    "# Your code setting up data structures here if necessary (equivalent to setup() function)\n",
    "table = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Your code operating on each line here (equivalent to map() function)\n",
    "    print(f'{line}', file=sys.stderr)\n",
    "    name, score_sum, count = line.strip().split('\\t')\n",
    "    score_sum = int(score_sum)\n",
    "    count = int(count)\n",
    "    if name not in table:\n",
    "        table[name] = [score_sum, count]\n",
    "    else:\n",
    "        table[name][0] += score_sum\n",
    "        table[name][1] += count\n",
    "\n",
    "# Your code for post-processing here if necessary (equivalent to cleanup() function)\n",
    "for name, [score_sum, count] in table.items():\n",
    "    print(f\"{name}\\t{score_sum/count}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa61579",
   "metadata": {},
   "source": [
    "## Define combiner (part of mapper) combine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c2267f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%file combiner.py\n",
    "import sys\n",
    "\n",
    "# Your code setting up data structures here if necessary (equivalent to setup() function)\n",
    "table = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Your code operating on each line here (equivalent to map() function)\n",
    "    print(f'{line}', file=sys.stderr)\n",
    "    name, score_sum, count = line.strip().split('\\t')\n",
    "    score_sum = int(score_sum)\n",
    "    count = int(count)\n",
    "    if name not in table:\n",
    "        table[name] = [score_sum, count]\n",
    "    else:\n",
    "        table[name][0] += score_sum\n",
    "        table[name][1] += count\n",
    "\n",
    "# Your code for post-processing here if necessary (equivalent to cleanup() function)\n",
    "for name, [score_sum, count] in table.items():\n",
    "    print(f\"{name}\\t{score_sum}\\t{count}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "266041a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+rwx mapper.py\n",
    "!chmod u+rwx reducer.py\n",
    "!chmod u+rwx combiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe836425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting doc.txt\n"
     ]
    }
   ],
   "source": [
    "%%file doc.txt\n",
    "jacob 88\n",
    "jacob 89\n",
    "yanlin 79\n",
    "tom 92\n",
    "jacob 82\n",
    "yanlin 89\n",
    "john 65\n",
    "john 87\n",
    "jacob 95\n",
    "yanlin 91\n",
    "tommy 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99446120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/neteasegames/Desktop/hadoop_mapreduce/average_score\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_directory = os.path.abspath(os.getcwd())\n",
    "print(\"Notebook directory:\", notebook_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aacea5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.join(notebook_directory, 'output')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f77af596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-01 13:28:37,735 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "2024-05-01 13:28:37,857 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/neteasegames/Desktop/hadoop_mapreduce/average_score/mapper.py, /Users/neteasegames/Desktop/hadoop_mapreduce/average_score/reducer.py, /Users/neteasegames/Desktop/hadoop_mapreduce/average_score/combiner.py, /var/folders/j4/j3x8v2856p95gl278v7rl4j80000gn/T/hadoop-unjar9911872606838456846/] [] /var/folders/j4/j3x8v2856p95gl278v7rl4j80000gn/T/streamjob7186285699448689803.jar tmpDir=null\n",
      "2024-05-01 13:28:38,401 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-01 13:28:38,514 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-05-01 13:28:38,696 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/neteasegames/.staging/job_1714541055633_0004\n",
      "2024-05-01 13:28:39,479 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-05-01 13:28:40,375 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-05-01 13:28:40,941 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1714541055633_0004\n",
      "2024-05-01 13:28:40,941 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-05-01 13:28:41,077 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-05-01 13:28:41,077 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-05-01 13:28:41,119 INFO impl.YarnClientImpl: Submitted application application_1714541055633_0004\n",
      "2024-05-01 13:28:41,138 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1714541055633_0004/\n",
      "2024-05-01 13:28:41,138 INFO mapreduce.Job: Running job: job_1714541055633_0004\n",
      "2024-05-01 13:28:46,238 INFO mapreduce.Job: Job job_1714541055633_0004 running in uber mode : false\n",
      "2024-05-01 13:28:46,240 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-05-01 13:28:50,346 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-05-01 13:28:55,400 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-05-01 13:28:56,427 INFO mapreduce.Job: Job job_1714541055633_0004 completed successfully\n",
      "2024-05-01 13:28:56,537 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=246\n",
      "\t\tFILE: Number of bytes written=941663\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=246\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=2\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4173\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1724\n",
      "\t\tTotal time spent by all map tasks (ms)=4173\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1724\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4173\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1724\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4273152\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1765376\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=11\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=120\n",
      "\t\tMap output materialized bytes=105\n",
      "\t\tInput split bytes=246\n",
      "\t\tCombine input records=11\n",
      "\t\tCombine output records=7\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=105\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=805306368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=147\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=78\n",
      "2024-05-01 13:28:56,537 INFO streaming.StreamJob: Output directory: file:///Users/neteasegames/Desktop/hadoop_mapreduce/average_score/output\n"
     ]
    }
   ],
   "source": [
    "# brew install hadoop\n",
    "!hadoop jar /opt/homebrew/Cellar/hadoop/3.4.0/libexec/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-input file://{os.path.join(notebook_directory, 'doc.txt')} \\\n",
    "-output file://{os.path.join(notebook_directory, 'output')} \\\n",
    "-file {os.path.join(notebook_directory, 'mapper.py')} \\\n",
    "-file {os.path.join(notebook_directory, 'reducer.py')} \\\n",
    "-file {os.path.join(notebook_directory, 'combiner.py')} \\\n",
    "-mapper '/Users/neteasegames/anaconda3/bin/python mapper.py' \\\n",
    "-reducer '/Users/neteasegames/anaconda3/bin/python reducer.py' \\\n",
    "-combiner '/Users/neteasegames/anaconda3/bin/python combiner.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "087f59bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacob\t88.5\r\n",
      "john\t76.0\r\n",
      "tom\t92.0\r\n",
      "tommy\t77.0\r\n",
      "yanlin\t86.33333333333333\r\n"
     ]
    }
   ],
   "source": [
    "# 00000 is the reducer\n",
    "!cat output/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954fe773",
   "metadata": {},
   "source": [
    "## Mapper 1 Combiner Input\n",
    "jacob\t82\t1\n",
    "\n",
    "jacob\t89\t1\n",
    "\n",
    "jacob\t88\t1\n",
    "\n",
    "tom\t92\t1\n",
    "\n",
    "yanlin\t89\t1\n",
    "\n",
    "yanlin\t79\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed294b53",
   "metadata": {},
   "source": [
    "## Mapper 2 Combiner Input\n",
    "jacob\t95\t1\n",
    "\n",
    "john\t87\t1\n",
    "\n",
    "john\t65\t1\n",
    "\n",
    "tommy\t77\t1\n",
    "\n",
    "yanlin\t91\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4093a283",
   "metadata": {},
   "source": [
    "## Reducer input\n",
    "jacob\t259\t3\n",
    "\n",
    "jacob\t95\t1\n",
    "\n",
    "john\t152\t2\n",
    "\n",
    "tom\t92\t1\n",
    "\n",
    "tommy\t77\t1\n",
    "\n",
    "yanlin\t91\t1\n",
    "\n",
    "yanlin\t168\t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362d8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
